{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import t, gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = '#2B8EE3', '#F9B43C', '#FA672F', '#BE1E2D', '#A7A9AC','#2B8EE3', '#F9B43C', '#FA672F', '#BE1E2D', '#A7A9AC'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "colors = [d['color'] for d in list(plt.rcParams['axes.prop_cycle'])]\n",
    "\n",
    "if os.path.exists('font/'):    \n",
    "    prop = fm.FontProperties(fname=f'font/micross.ttf')\n",
    "    mpl.rcParams['font.family'] = prop.get_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = json.loads(open('data/features_sets_info.json','r').read())\n",
    "feature_sets_map = dict(zip(feature_sets['labels'], feature_sets['labels_exhaust']))\n",
    "feature_sets_map_expand = dict(zip(feature_sets['labels_expand'], feature_sets['labels_exhaust_expand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection and label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = dict(\n",
    "    lr_p1 = \"(clf=='LR')&(fs==True)&(poly_degree==1)&(bins=='3 - uneven')\",\n",
    "    lr_p2 = \"(clf=='LR')&(fs==True)&(poly_degree==2)&(bins=='3 - uneven')\",    \n",
    "    rf_t_fs = \"(clf=='RF')&(estimate_hyper_param==True)&(fs==True)\",\n",
    "    rf_t_nfs = \"(clf=='RF')&(estimate_hyper_param==True)&(fs==False)\")\n",
    "\n",
    "model_map = dict(lr_p1=('Logistic Regression', 'polynom. order=1'),\n",
    "                 lr_p2=('Logistic Regression', 'polynom. order=2'),           \n",
    "                 rf_t_fs=('Random forest', 'with feat. select.'),\n",
    "                 rf_t_nfs=('Random forest', 'w/o feat. select.'))\n",
    "\n",
    "for max_d, mss in [(20,5),(10,5),(50,5),(20,2),(20,10)]:\n",
    "    key = f'rf_nt_{max_d}md_{mss}mss'     \n",
    "    model_select[key] = \\\n",
    "        f\"(clf=='RF')&(estimate_hyper_param==False)&(clf_max_depth=={max_d})&(clf_min_samples_split=={mss})\"\n",
    "    model_map[key]=\\\n",
    "        'Random forest', f'fixed hyperparam. (max_depth,{max_d}; min_sample_split,{mss})'\n",
    "    \n",
    "for bins in 3,4,5:\n",
    "    key = f'lr_p1_even{bins}'\n",
    "    model_select[key] = f'bins==\"{bins}\"'\n",
    "    model_map[key] = f'Log. regression', f'pol. ord.=1; {bins} even bin target' \n",
    "\n",
    "model_process = ['lr_p1','lr_p2','rf_nt_20md_5mss','rf_t_nfs', 'lr_p1_even3', 'lr_p1_even4']\n",
    "models_plot_agg = {k:v for k,v in model_select.items() if k in model_process}\n",
    "\n",
    "model_label_exhaust = {k:', '.join(v) for k,v in model_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model comparison information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature groupings\n",
    "feature_groups = dict(\n",
    "    major = ['combined_all','pre_all','post_all'],\n",
    "    expanding = ['pre_all', 'pre_elem_and_elem','pre_elem'],\n",
    "    task=['task', 'non_task'],\n",
    "    all_feat_sets=feature_sets['labels'])\n",
    "\n",
    "feature_group_letter = dict(major='A',expanding='C',task='B')\n",
    "\n",
    "comparison_sets = dict(\n",
    "    major = [('pre_all', 'post_all'), ('pre_all', 'combined_all')],\n",
    "    expanding = [('pre_elem', 'pre_elem_and_elem'), ('pre_elem_and_elem', 'pre_all')],\n",
    "    all_feat_sets = [],\n",
    "    task=[('non_task','task')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_models = pd.read_csv('data/models_a.csv').append(pd.read_csv('data/models_b.csv'))\n",
    "data_bins = pd.read_csv('data/bins.csv')\n",
    "\n",
    "data = pd.concat([data_models, data_bins.query('bins!=\"3 - uneven\"')],sort=False).copy()\n",
    "data.bins = data.bins.fillna('3 - uneven')\n",
    "\n",
    "\n",
    "data['feature_set_label_exhaust'] = \\\n",
    "    pd.Categorical(data.feature_set_label.map(feature_sets_map),\n",
    "                   categories=feature_sets['labels_exhaust'],\n",
    "                   ordered=True)\n",
    "for acronym, selection in model_select.items():    \n",
    "    data.loc[data.query(selection).index, 'Model'] = ' - '.join(model_map[acronym])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate table and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = {}\n",
    "comparison_df = {model_label_exhaust[v]:{} for v in models_plot_agg.keys()}\n",
    "acc_dict = {}\n",
    "\n",
    "models_plot = {k:v for k,v in model_select.items() if k in models_plot_agg.keys()}\n",
    "\n",
    "for model_label, selection in models_plot.items():\n",
    "\n",
    "    mle = model_label_exhaust[model_label]\n",
    "    mean_acc[mle]=\\\n",
    "        data\\\n",
    "            .query(selection)\\\n",
    "            .groupby('feature_set_label_exhaust')\\\n",
    "            .accuracy\\\n",
    "            .mean()\n",
    "    \n",
    "    \n",
    "    acc_gb_fs = \\\n",
    "        data\\\n",
    "        .query(selection)\\\n",
    "        .groupby('feature_set_label_exhaust').accuracy\n",
    "    \n",
    "    acc_fs_agg =acc_gb_fs.agg(['mean', 'std'])\n",
    "    acc_fs_agg.index = \\\n",
    "        [s.replace('\\n', ' ') for s in acc_fs_agg.index.tolist()]\n",
    "    acc_dict[mle] = acc_fs_agg\n",
    "    \n",
    "    for feature_group, comparison_set_ in feature_groups.items():\n",
    "        \n",
    "        label = '%s_%s' % (feature_group, model_label)\n",
    "\n",
    "        ####################\n",
    "        # data structuring #\n",
    "        ####################\n",
    "        compare_selection = data.feature_set_label.isin(comparison_set_)        \n",
    "        df = data.loc[compare_selection].query(selection).copy()\n",
    "\n",
    "        # features\n",
    "        fs_map = feature_sets_map\n",
    "        fs_exhaust = feature_sets['labels_exhaust']\n",
    "        if feature_group=='expanding':\n",
    "            fs_map = feature_sets_map_expand\n",
    "            fs_exhaust = feature_sets['labels_exhaust_expand']\n",
    "\n",
    "        fs_exhaust_sub = df.feature_set_label.map(fs_map)\n",
    "\n",
    "        df['fs_long'] = \\\n",
    "            pd.Categorical(fs_exhaust_sub,\n",
    "                           categories=[f for f in fs_exhaust if f in fs_exhaust_sub.unique()],\n",
    "                           ordered=True)\n",
    "        df_idx = df.set_index('feature_set_label')\n",
    "\n",
    "        # table agg\n",
    "        for set1, set2 in comparison_sets[feature_group]:\n",
    "            diff = df_idx.loc[set2].accuracy.values-df_idx.loc[set1].accuracy.values\n",
    "            mu, std, corr = diff.mean(), diff.std(), np.sqrt(1/1000+1/3)\n",
    "            t_corr = mu/(corr*std)\n",
    "            p_corr = t.sf(np.abs(t_corr),1000-1)*2\n",
    "            comparison_pair = '%s vs %s' % (feature_sets_map[set1], feature_sets_map[set2])\n",
    "            cp = comparison_pair.replace('\\n', ' ')\n",
    "            comparison_df[mle][cp] = '%.2f (p=%.4f)'% (mu, p_corr)\n",
    "\n",
    "\n",
    "        # figure output\n",
    "        figpath = 'output/%s_letter.png' % label\n",
    "        if not os.path.exists(figpath):\n",
    "            has_x_label = feature_group == 'expanding'#in ('task', 'major')\n",
    "            f,ax = plt.subplots(figsize=(6,0.8+0.6*len(comparison_set_)+0.2*has_x_label))\n",
    "\n",
    "            sub = df.reset_index().pipe(lambda df: df[df.feature_set_label.isin(comparison_set_)])\n",
    "\n",
    "            if feature_group == 'expanding':\n",
    "                colors_ = sns.light_palette(colors[1],n_colors=4)[1:][::-1]        \n",
    "            elif feature_group == 'task':\n",
    "                colors_ = colors[3:]\n",
    "            elif feature_group == 'major':\n",
    "                colors_ = [colors[i] for i in  [2,1,0]]\n",
    "            else:\n",
    "                colors_ = colors\n",
    "\n",
    "            # make KDE part\n",
    "            n = len(comparison_set_)\n",
    "            pos = list(range(1,n+1))\n",
    "            accs = [sub[sub.feature_set_label==fs].accuracy.values for fs in comparison_set_]\n",
    "            widths = [] \n",
    "            for fs in feature_groups[feature_group]:\n",
    "                acc = df[df.feature_set_label==fs].accuracy\n",
    "                kde = gaussian_kde(acc)\n",
    "                widths.append(kde(acc).max()/12)\n",
    "            parts = ax.violinplot(accs, pos, widths=widths, vert=False, showextrema=False) \n",
    "            ax.set_yticks(pos)\n",
    "            ax.set_yticklabels([fs_map[fs] for fs in comparison_set_])\n",
    "            for i, pc in enumerate(parts['bodies']):\n",
    "                if feature_group!='all_feat':\n",
    "                    pc.set_color(colors_[i])\n",
    "                pc.set_edgecolor('black')\n",
    "                pc.set_alpha(1)\n",
    "\n",
    "            # make box plot part (quantiles)\n",
    "            m,q05,q10,q25,q50,q75,q90,q95,M = np.quantile(accs,[0,.05,.1,.25,.5,.75,.9,.95,1],1)\n",
    "            ax.hlines(pos, q10, q90, color='k', lw=1.5)\n",
    "            ax.hlines(pos, q25, q75, color='k', lw=7)\n",
    "            ax.scatter(q50,pos,color='white',marker='o',s=20,zorder=3)\n",
    "\n",
    "            # shared\n",
    "            ax.set_ylabel('')\n",
    "            \n",
    "            if model_label == 'lr_p1_even4':\n",
    "                ax.axvline(x=0.25, color='gray',linestyle='--')\n",
    "            elif model_label == 'lr_p1_even5':    \n",
    "                ax.axvline(x=0.2, color='gray',linestyle='--')\n",
    "            else:\n",
    "                ax.axvline(x=0.3333, color='gray',linestyle='--')\n",
    "                \n",
    "            ax.set_xlim(0.2, 0.8)\n",
    "            if has_x_label:\n",
    "                ax.set_xlabel('Balanced accuracy')\n",
    "                plt.subplots_adjust(bottom=.2)\n",
    "            else:\n",
    "                ax.set_xlabel('')\n",
    "            ax.yaxis.grid(False)    \n",
    "\n",
    "            plt.subplots_adjust(left=.24)\n",
    "\n",
    "            if feature_group in feature_group_letter:\n",
    "                ax.text(0.03,0.33+n, feature_group_letter[feature_group], size=24)\n",
    "\n",
    "            f.savefig(figpath)\n",
    "            f.savefig(figpath.replace('png','pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate plots of hyperparameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(feature_sets['labels'][:3]):    \n",
    "    figpath = f'output/lr_hyperparams_{label}.png'\n",
    "    if not os.path.exists(figpath):\n",
    "        f,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "        ax[0].set_xlabel('Ridge parameter, log scaled (=$\\log_{10}(\\lambda_2))$')\n",
    "        ax[1].set_xlabel('Number of features used')\n",
    "        plotdata_ = data_models.query(f'(feature_set_label==\"{label}\")&(clf==\"LR\")&(poly_degree==1)')\n",
    "        plotdata_\\\n",
    "            .loc[:,'hyperparam_clf__C']\\\n",
    "            .pipe(np.log10)\\\n",
    "            .plot\\\n",
    "            .hist(ax=ax[0],bins=20)\n",
    "        plotdata_\\\n",
    "            .loc[:,'hyperparam_fs__max_features']\\\n",
    "            .value_counts()\\\n",
    "            .sort_index()\\\n",
    "            .plot(ax=ax[1])\n",
    "\n",
    "        ax[1].set_xlim(1,31)\n",
    "\n",
    "        f.suptitle(feature_sets['labels_exhaust'][i])\n",
    "    \n",
    "        f.savefig(figpath)\n",
    "        f.savefig(figpath.replace('png','pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate accuracy across models and statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_alt_classifier = [model_label_exhaust[m] for m in ['lr_p1','lr_p2', 'rf_nt_20md_5mss','rf_t_nfs']]\n",
    "labels_alt_target = [model_label_exhaust[m] for m in ['lr_p1', 'lr_p1_even3', 'lr_p1_even4']]\n",
    "\n",
    "# statististical tests overview \n",
    "comparison_tab = pd.DataFrame(comparison_df)\n",
    "comparison_tab[labels_alt_classifier].to_csv('output/comparison_tab_classifiers.csv')\n",
    "comparison_tab[labels_alt_target].to_csv('output/comparison_tab_targets.csv')\n",
    "\n",
    "\n",
    "# accuracy overview - different models\n",
    "acc_tab = pd.concat(acc_dict,axis=1).round(3)\n",
    "acc_tab.columns = \\\n",
    "    pd.MultiIndex.from_arrays(arrays=[acc_tab.columns.get_level_values(0),\n",
    "                                      ['Mean', 'Std. Dev.']*len(acc_dict)],\n",
    "                              names=['Model', 'Feature set'])\n",
    "acc_tab[labels_alt_classifier].to_csv('output/acc_tab_classifiers.csv')\n",
    "acc_tab[labels_alt_target].to_csv('output/acc_tab_targets.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
